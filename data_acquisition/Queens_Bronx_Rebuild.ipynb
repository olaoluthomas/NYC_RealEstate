{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I'm also rebuilding Queens and Bronx separately...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "# from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "# from retrying import retry\n",
    "import tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_type = []\n",
    "lat = []\n",
    "lng = []\n",
    "address = []\n",
    "no_of_beds = []\n",
    "no_of_baths = []\n",
    "sq_area = []\n",
    "n_hood = []\n",
    "price = []\n",
    "\n",
    "url = 'http://streeteasy.com/for-sale/queens/status:listed?refined_search=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def page_scrape(page):\n",
    "    # count = 0\n",
    "    # On slow connections...\n",
    "    # result = WebDriverWait(page, 30).until(EC.presence_of_element_located((By.ID, 'result-details')))\n",
    "    # listings = result.find_element_by_tag_name('ul').find_element_by_tag_name('li')\n",
    "    listings = page.find_element_by_id('result-details').find_element_by_tag_name('ul').find_elements_by_tag_name('li')\n",
    "    # collect data here by iterating through each listing and appending to our lists\n",
    "    for l in listings[:14]:\n",
    "        # I need an IF statement to test whether the listing is legit before scraping to reduce the amount of N/A values\n",
    "        # Initiating a counter to help identify at what listing the code breaks, if it does...\n",
    "        # This has become redundant with the introduction of tenacity retry function\n",
    "        # count +=1\n",
    "        \n",
    "        # longitude and latitude\n",
    "        g = None\n",
    "        try:\n",
    "            g = l.get_attribute('se:map:point')\n",
    "            if g:\n",
    "                lt, ln = g.split(',')\n",
    "                lat.append(float(lt))\n",
    "                lng.append(float(ln))\n",
    "            else:\n",
    "                lat.append('N/A')\n",
    "                lng.append('N/A')\n",
    "        except:\n",
    "            lat.append('N/A')\n",
    "            lng.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # address\n",
    "        ad = None\n",
    "        try:\n",
    "            ad = l.find_element_by_class_name('details-title').text.split('\\n')[0]\n",
    "            if ad:\n",
    "                address.append(ad)\n",
    "            else:\n",
    "                address.append('N/A')\n",
    "        except:\n",
    "            address.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # price\n",
    "        p = None\n",
    "        try:\n",
    "            p = float(l.find_element_by_class_name('price').text.replace('$','').replace(',', ''))\n",
    "            if p:\n",
    "                price.append(p)\n",
    "            else:\n",
    "                price.append('N/A')\n",
    "        except:\n",
    "            price.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # number of beds\n",
    "        bd_detail = None\n",
    "        try:\n",
    "            bd_detail = l.find_element_by_class_name('details_info').find_element_by_tag_name('span')\n",
    "            if bd_detail.text.find('bed') > 0:\n",
    "                no_of_beds.append(float(bd_detail.text.split(' ')[0]))\n",
    "            # do we want this as a string or float? what are the regression/ml requirements?\n",
    "            else:\n",
    "                no_of_beds.append('N/A')\n",
    "        except:\n",
    "            no_of_beds.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # number of baths\n",
    "        baths = None\n",
    "        try:\n",
    "            lstn_details = l.find_element_by_class_name('details_info').find_elements_by_tag_name('span')\n",
    "            for detail in lstn_details:\n",
    "                if detail.text.find('bath') > 0:\n",
    "                    try:\n",
    "                        baths = float(detail.text.split(' ')[0])\n",
    "                    except:\n",
    "                        baths = 'N/A'\n",
    "        except:\n",
    "            baths = 'N/A'\n",
    "        no_of_baths.append(baths)\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # square area NB: value in previous listing is being appended to next listing. FIX!\n",
    "        # update: fixed.\n",
    "        area = None\n",
    "        try:\n",
    "            l_details = l.find_element_by_class_name('details_info').find_elements_by_tag_name('span')\n",
    "            for detail in l_details:\n",
    "                 if detail.text.find('ft') > 0:\n",
    "                    area = float(detail.text.split(' ')[0].replace(',', ''))\n",
    "            if area:\n",
    "                sq_area.append(area)\n",
    "            else:\n",
    "                sq_area.append('N/A')\n",
    "        except:\n",
    "            sq_area.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # listing type and neighborhood\n",
    "        l_type = None\n",
    "        nhood = None\n",
    "        try:\n",
    "            area_details = l.find_elements_by_class_name('details_info')[1].text\n",
    "            l_type, nhood = area_details.split(' in ')\n",
    "            if l_type:\n",
    "                listing_type.append(l_type)\n",
    "            else:\n",
    "                listing_type.append('N/A')\n",
    "            if nhood:\n",
    "                n_hood.append(nhood)\n",
    "            else:\n",
    "                n_hood.append('N/A')\n",
    "        except:\n",
    "            listing_type.append('N/A')\n",
    "            n_hood.append('N/A')\n",
    "        # time.sleep(1)\n",
    "    # if count == 14:\n",
    "        # print('Moving on to the next page...')\n",
    "    # streeteasy introduces a captcha when they suspect scraping. How will this be overridden?\n",
    "    # fixed by using Firefox in place of Chrome\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_page():\n",
    "    nxt = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'next')))\n",
    "    # formerly: nxt = listns[-1].find_element_by_class_name('next'); WebDriverWait improves stability\n",
    "    nxt.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@tenacity.retry()\n",
    "def retry():\n",
    "    driver.get(driver.current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying...\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Firefox()\n",
    "page_crash = 0\n",
    "driver.get(url)\n",
    "time.sleep(2) # possible crash here without time.sleep?\n",
    "listns = page_scrape(driver)\n",
    "last_page = int(listns[-1].find_elements_by_class_name('page')[-1].text)\n",
    "# print 'Counter:', counter\n",
    "next_page()\n",
    "for i in range(1, last_page):\n",
    "    try:\n",
    "        # counter += 1 # increment repetition may occur here if a break occurs in this try loop... avoided by using scraping in except loop\n",
    "        listns = page_scrape(driver)\n",
    "        next_page()\n",
    "    except:\n",
    "        page_crash += 1\n",
    "        print 'Retrying...'\n",
    "        retry()\n",
    "    finally:\n",
    "        delay = random.uniform(0.5, 1)\n",
    "        time.sleep(delay)\n",
    "driver.close()\n",
    "dic = {'building type':listing_type, 'latitude':lat, 'longitude':lng, 'address':address, 'beds':no_of_beds, 'baths':no_of_baths, 'area':sq_area, 'neighborhood':n_hood, 'price':price}\n",
    "data = pd.DataFrame(dic)\n",
    "# data.to_csv('Queens.csv', index=False)\n",
    "data.to_csv('Bronx.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_type = []\n",
    "lat = []\n",
    "lng = []\n",
    "address = []\n",
    "no_of_beds = []\n",
    "no_of_baths = []\n",
    "sq_area = []\n",
    "n_hood = []\n",
    "price = []\n",
    "\n",
    "url = 'http://streeteasy.com/for-sale/bronx/status:listed?refined_search=true'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
