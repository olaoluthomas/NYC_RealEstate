{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ML to Predict NYC Real Estate Value & Investment Opportunity\n",
    "\n",
    "*This notebook scrapes streeteasy.com for data on listings for sale in the Five Boroughs and applies ML to evaluate my predictive model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. what are the requirements?\n",
    "# from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "# from collections import OrderedDict\n",
    "from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boroughs = ['manhattan','brooklyn','queens','bronx','staten-island']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_def(lst):\n",
    "    lst_of_urls = []\n",
    "    for item in lst:\n",
    "        lst_of_urls.append('http://streeteasy.com/for-sale/'+str(item)+'/status:listed?refined_search=true')\n",
    "    return lst_of_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://streeteasy.com/for-sale/manhattan/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/brooklyn/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/queens/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/bronx/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/staten-island/status:listed?refined_search=true']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = url_def(boroughs)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to visually inspect all generated urls...\n",
    "# ?! this doesn't make sense! fix it!\n",
    "'''\n",
    "resultas = []\n",
    "for url in urls:\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "    except:\n",
    "        \n",
    "    driver.close()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "What would be the most useful features to collect for this project?\n",
    "\n",
    "- **Type of house, Location of house, Neighborhood, Number of rooms, number of baths**, availability of amenities in building (laundry, doorman, super)?, proximity to transit, proximity to waterfront, (with the Price of house as target).\n",
    "\n",
    "Features in bold are available on streeteasy...\n",
    "\n",
    "Q. ***Can any useful features be engineered from those available or retrieved from an alternate source?***\n",
    "\n",
    "### Scraped features\n",
    "\n",
    "The features available from Streeteasy.com are:\n",
    "\n",
    " - House type, Geo-location, House address, No. of beds, No. of baths, Square area of house, Neighborhood, Price\n",
    " \n",
    "*What features can be derived from these? What additional insight will these derived features provide?*\n",
    "*Can more useful features be retrieved from other sources to complement Streeteasy?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_id = []\n",
    "listing_type = []\n",
    "lat_long = []\n",
    "address = []\n",
    "no_of_beds = []\n",
    "no_of_baths = []\n",
    "sq_area = []\n",
    "n_hood = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Thoughts*\n",
    "\n",
    "Do the project requirements dictate the statistical method/algorithm used? Will these, in turn, determine whether categorical or continuous variables are required?\n",
    "\n",
    "- *Linear Regression*\n",
    "- *Logistic Regression*\n",
    "- *Random forest*\n",
    "\n",
    "Number of beds, baths, square area can be either categorical or continuous...\n",
    "\n",
    "**N.B. This requirement directly dictates the page_scrape function below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def page_scrape(page):\n",
    "    count = 0\n",
    "    # time.sleep(5)\n",
    "    listings = page.find_element_by_xpath('//*[@id=\"result-details\"]/ul').find_elements_by_tag_name('li')\n",
    "    # collect data here by iterating through each listing and appending to our lists\n",
    "    for l in listings[:14]:\n",
    "        # initiate a counter to help identify at what listing the code breaks, if it does...\n",
    "        # also, a counter for the number of pages scraped should be implemented in the function that navigates pages\n",
    "        count +=1\n",
    "        \n",
    "        # longitude and latitude\n",
    "        g = l.get_attribute('se:map:point')\n",
    "        if g:\n",
    "            lat_long.append(g)\n",
    "        else:\n",
    "            lat_long.append('N/A')\n",
    "        g = None\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # address\n",
    "        ad = l.find_element_by_class_name('details-title').text.split('\\n')[0]\n",
    "        if ad:\n",
    "            address.append(ad)\n",
    "        else:\n",
    "            address.append('N/A')\n",
    "        ad = None\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # price\n",
    "        p = float(l.find_element_by_class_name('price').text.replace('$','').replace(',', ''))\n",
    "        if p:\n",
    "            price.append(p)\n",
    "        else:\n",
    "            price.append('N/A')\n",
    "        p = None\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # number of beds\n",
    "        bd_detail = l.find_element_by_class_name('details_info').find_element_by_tag_name('span')\n",
    "        if bd_detail.text.find('bed') > 0:\n",
    "            no_of_beds.append(float(bd_detail.text.split(' ')[0]))\n",
    "            # do we want this as a string or float? what are the regression/ml requirements?\n",
    "        else:\n",
    "            no_of_beds.append('N/A')\n",
    "        bd_detail = None\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # number of baths\n",
    "        lstn_details = l.find_element_by_class_name('details_info').find_elements_by_tag_name('span')\n",
    "        for detail in lstn_details:\n",
    "            if detail.text.find('bath') > 0:\n",
    "                baths = float(detail.text.split(' ')[0])\n",
    "            if detail.text.find('ft') > 0:\n",
    "                area = float(detail.text.split(' ')[0].replace(',', ''))\n",
    "        if baths:\n",
    "            no_of_baths.append(baths)\n",
    "        else:\n",
    "            no_of_baths.append('N/A')\n",
    "        baths = None\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # square area NB: value in previous listing is being appended to next listing. FIX!\n",
    "        # update: fixed.\n",
    "        if area:\n",
    "            sq_area.append(area)\n",
    "        else:\n",
    "            sq_area.append('N/A')\n",
    "        area = None\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # listing type and neighborhood\n",
    "        area_details = l.find_elements_by_class_name('details_info')[1].text\n",
    "        l_type = area_details.split(' in ')[0].strip(' ')\n",
    "        nhood = area_details.split(' in ')[1].strip(' ')\n",
    "            \n",
    "        if l_type:\n",
    "            listing_type.append(l_type)\n",
    "            # a spell-checker is required to correct mispells in house type e.g. 'Condop' instead of 'Condo'\n",
    "        else:\n",
    "            listing_type.append('N/A')\n",
    "        if nhood:\n",
    "            n_hood.append(nhood)\n",
    "        else:\n",
    "            n_hood.append('N/A')\n",
    "        l_type = None\n",
    "        nhood = None\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print count\n",
    "    if count == 14:\n",
    "        print('Moving on to the next page...')\n",
    "    # streeteasy introduces a captcha when they suspect scraping. How will this be overridden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling 'N/A' values and outliers...\n",
    "\n",
    "*Are samples with missing data discarded or replaced with the feature median? What is the norm as pertains to this situation...?*\n",
    "\n",
    "Are statistical outliers really outliers in this use case? (Yes/**No**)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_page(page):\n",
    "    last_page = int(page.find_element_by_xpath('//*[@id=\"result-details\"]/ul/li[17]/nav/span[10]').text)\n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_page(page):\n",
    "    url = driver.current_url\n",
    "    split_url = \n",
    "        print(\"You're on the next page...\")\n",
    "    except:\n",
    "        print('You have reached the last page...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Moving on to the next page...\n"
     ]
    }
   ],
   "source": [
    "page_scrape(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for float(): 2,467",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-905c21669601>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpage_scraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# time.sleep(5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# nxt_page(driver)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-dd081fba400f>\u001b[0m in \u001b[0;36mpage_scraper\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mbaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdetail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ft'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0marea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mno_of_baths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for float(): 2,467"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "time.sleep(2)\n",
    "driver.get(urls[0])\n",
    "time.sleep(5)\n",
    "\n",
    "page_scrape(driver)\n",
    "# time.sleep(5)\n",
    "# nxt_page(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 3.0, 3.0, 3.0, 1.0, 3.0, 4.0, 2.0, 3.0, 1.0, 2.0, 3.0, 1.0, 'N/A', 'N/A', 'N/A']\n",
      "[u'45 East 22nd Street #46A', u'45 East 22nd Street #46A', u'45 East 22nd Street #46A', u'2 River Terrace #14N', u'453 Fdr Drive #C1703', u'45 Park Place #38', u'45 Park Place #24', u'45 Park Place #19W', u'45 Park Place #7E', u'200 East End Avenue #5N', u'1060 Fifth Avenue #10C', u'34 Gramercy Park East #8AR', u'435 East 85th Street #4MN', u'240 East 76th Street #11P', u'77 Bleecker Street #214', u'98 Park Terrace East #2H']\n",
      "[3.0, 3.0, 1.0, 3.5, 4.5, 2.5, 3.5, 1.0, 2.5, 2.0, 1.5, 1.0, 1.0, 1.0]\n",
      "[u'40.73989868,-73.98729706', u'40.73989868,-73.98729706', u'40.73989868,-73.98729706', u'40.71559906,-74.01609802', u'40.71289825,-73.97889709', u'40.71379852,-74.00990295', u'40.71379852,-74.00990295', u'40.71379852,-74.00990295', u'40.71379852,-74.00990295', u'40.77730179,-73.9434967', u'40.782321,-73.959613', u'40.73730087,-73.98500061', u'40.77569962,-73.94809723', u'40.77119827,-73.95700073', u'40.72710037,-73.99590302', u'40.87110138,-73.91529846']\n",
      "[9375000.0, 9375000.0, 9375000.0, 3595000.0, 699000.0, 12350000.0, 10500000.0, 3945000.0, 4745000.0, 765000.0, 5995000.0, 3450000.0, 750000.0, 555000.0, 549000.0, 249000.0]\n",
      "[2467.0, 1580.0, 800.0, 3238.0, 3562.0, 1565.0, 1945.0, 'N/A', 2600.0, 1900.0, 750.0, 550.0, 'N/A', 450.0]\n",
      "[u'Condo', u'Condo', u'Co-op', u'Condo', u'Condo', u'Condo', u'Condo', u'Co-op', u'Co-op', u'Co-op', u'Co-op', u'Condop', u'Co-op', u'Co-op']\n",
      "[u'Flatiron', u'Battery Park City', u'Lower East Side', u'Tribeca', u'Tribeca', u'Tribeca', u'Tribeca', u'Yorkville', u'Carnegie Hill', u'Gramercy Park', u'Yorkville', u'Upper East Side', u'Greenwich Village', u'Inwood']\n"
     ]
    }
   ],
   "source": [
    "dic = {'type':listing_type, 'geo':lat_long, 'addr':address, 'beds':no_of_beds, 'baths':no_of_baths, 'area':sq_area, 'hood':n_hood, 'price':price}\n",
    "for key in dic:\n",
    "    print dic[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have reached the last page...\n"
     ]
    }
   ],
   "source": [
    "nxt_page(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_page = int(driver.find_element_by_xpath('//*[@id=\"result-details\"]/ul/li[17]/nav/span[10]').text)\n",
    "last_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"class name\",\"selector\":\"next\"}\n  (Session info: chrome=58.0.3029.110)\n  (Driver info: chromedriver=2.27.440174 (e97a722caafc2d3a8b807ee115bfb307f7d2cfd9),platform=Windows NT 10.0.14393 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-f944ecc1f703>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 3. how will we iterate over all results pages and repeat step 2?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnext_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'next'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_tag_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnext_page\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laolu\\Anaconda2\\envs\\py27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mfind_element_by_class_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'foo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \"\"\"\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laolu\\Anaconda2\\envs\\py27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    789\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0;32m    790\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laolu\\Anaconda2\\envs\\py27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    258\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\Users\\Laolu\\Anaconda2\\envs\\py27\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.pyc\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'alert'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"class name\",\"selector\":\"next\"}\n  (Session info: chrome=58.0.3029.110)\n  (Driver info: chromedriver=2.27.440174 (e97a722caafc2d3a8b807ee115bfb307f7d2cfd9),platform=Windows NT 10.0.14393 x86_64)\n"
     ]
    }
   ],
   "source": [
    "# 3. how will we iterate over all results pages and repeat step 2?\n",
    "try:\n",
    "    next_page = driver.find_element_by_class_name('next').find_element_by_tag_name('a')\n",
    "    next_page.click()\n",
    "except:\n",
    "    print('You have reached the last page...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_page.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exloratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set(style='whitegrid', context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Equations\n",
    "- Multiple Linear Regression:\n",
    " - $y = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum\\limits_{i=0}^{\\infty} w_ix_i = w^Tx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CClass in module numpy.lib.index_tricks object:\n",
      "\n",
      "class CClass(AxisConcatenator)\n",
      " |  Translates slice objects to concatenation along the second axis.\n",
      " |  \n",
      " |  This is short-hand for ``np.r_['-1,2,0', index expression]``, which is\n",
      " |  useful because of its common occurrence. In particular, arrays will be\n",
      " |  stacked along their last axis after being upgraded to at least 2-D with\n",
      " |  1's post-pended to the shape (column vectors made out of 1-D arrays).\n",
      " |  \n",
      " |  For detailed documentation, see `r_`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
      " |  array([[1, 2, 3, 0, 0, 4, 5, 6]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CClass\n",
      " |      AxisConcatenator\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from AxisConcatenator:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __getslice__(self, i, j)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from AxisConcatenator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "help(np.c_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
