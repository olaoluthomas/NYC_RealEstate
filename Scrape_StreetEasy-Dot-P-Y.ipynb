{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ML to Predict NYC Real Estate Value & Investment Opportunity\n",
    "\n",
    "*This notebook scrapes streeteasy.com for data on listings for sale in the Five Boroughs and applies ML to evaluate my predictive model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "# import json\n",
    "# import os\n",
    "# from collections import OrderedDict\n",
    "# from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boroughs = ['manhattan','brooklyn','queens','bronx','staten-island']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_def(lst):\n",
    "    lst_of_urls = []\n",
    "    for item in lst:\n",
    "        lst_of_urls.append('http://streeteasy.com/for-sale/'+str(item)+'/status:listed?refined_search=true')\n",
    "    return lst_of_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://streeteasy.com/for-sale/manhattan/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/brooklyn/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/queens/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/bronx/status:listed?refined_search=true',\n",
       " 'http://streeteasy.com/for-sale/staten-island/status:listed?refined_search=true']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = url_def(boroughs)\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "What would be the most useful features to collect for this project?\n",
    "\n",
    "- **Type of house, Location of house, Neighborhood, Number of rooms, number of baths**, availability of amenities in building (laundry, doorman, super)?, proximity to transit, proximity to waterfront, (with the Price of house as target).\n",
    "\n",
    "Features in bold are available on streeteasy...\n",
    "\n",
    "Q. ***Can any useful features be engineered from those available or retrieved from an alternate source?***\n",
    "\n",
    "### Scraped features\n",
    "\n",
    "The features available from Streeteasy.com are:\n",
    "\n",
    " - House type, Geo-location, House address, No. of beds, No. of baths, Square area of house, Neighborhood, Price\n",
    " \n",
    "*What features can be derived from these? What additional insight will these derived features provide?*\n",
    "*Can more useful features be retrieved from other sources to complement Streeteasy?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_type = []\n",
    "lat = []\n",
    "lng = []\n",
    "address = []\n",
    "no_of_beds = []\n",
    "no_of_baths = []\n",
    "sq_area = []\n",
    "n_hood = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Thoughts*\n",
    "\n",
    "Do the project requirements dictate the statistical method/algorithm used? Will these, in turn, determine whether categorical or continuous variables are required?\n",
    "\n",
    "- *Linear Regression*\n",
    "- *Logistic Regression*\n",
    "- *Random forest*\n",
    "\n",
    "**N.B. This requirement directly dictates the page_scrape function below.**\n",
    "\n",
    "**I'm going with numerical variables where possible...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def page_scrape(page):\n",
    "    count = 0\n",
    "    # On slow connections...\n",
    "    # listings = WebDriverWait(page, 30).until(EC.visibility_of_element_located((By.ID, 'results-details')))\n",
    "    listings = page.find_element_by_id('result-details').find_element_by_tag_name('ul').find_elements_by_tag_name('li')\n",
    "    # collect data here by iterating through each listing and appending to our lists\n",
    "    for l in listings[:14]:\n",
    "        # initiate a counter to help identify at what listing the code breaks, if it does...\n",
    "        count +=1\n",
    "        # print count\n",
    "        \n",
    "        # longitude and latitude\n",
    "        g = None\n",
    "        try:\n",
    "            g = l.get_attribute('se:map:point')\n",
    "            if g:\n",
    "                lt, ln = g.split(',')\n",
    "                lat.append(float(lt))\n",
    "                lng.append(float(ln))\n",
    "            else:\n",
    "                lat.append('N/A')\n",
    "                lng.append('N/A')\n",
    "        except:\n",
    "            lat.append('N/A')\n",
    "            lng.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # address\n",
    "        ad = None\n",
    "        try:\n",
    "            ad = l.find_element_by_class_name('details-title').text.split('\\n')[0]\n",
    "            if ad:\n",
    "                address.append(ad)\n",
    "            else:\n",
    "                address.append('N/A')\n",
    "        except:\n",
    "            address.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # price\n",
    "        p = None\n",
    "        try:\n",
    "            p = float(l.find_element_by_class_name('price').text.replace('$','').replace(',', ''))\n",
    "            if p:\n",
    "                price.append(p)\n",
    "            else:\n",
    "                price.append('N/A')\n",
    "        except:\n",
    "            price.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # number of beds\n",
    "        bd_detail = None\n",
    "        try:\n",
    "            bd_detail = l.find_element_by_class_name('details_info').find_element_by_tag_name('span')\n",
    "            if bd_detail.text.find('bed') > 0:\n",
    "                no_of_beds.append(float(bd_detail.text.split(' ')[0]))\n",
    "            # do we want this as a string or float? what are the regression/ml requirements?\n",
    "            else:\n",
    "                no_of_beds.append('N/A')\n",
    "        except:\n",
    "            no_of_beds.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # number of baths\n",
    "        baths = None\n",
    "        try:\n",
    "            lstn_details = l.find_element_by_class_name('details_info').find_elements_by_tag_name('span')\n",
    "            for detail in lstn_details:\n",
    "                if detail.text.find('bath') > 0:\n",
    "                    try:\n",
    "                        baths = float(detail.text.split(' ')[0])\n",
    "                    except:\n",
    "                        baths = 'N/A'\n",
    "        except:\n",
    "            baths = 'N/A'\n",
    "        no_of_baths.append(baths)\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # square area NB: value in previous listing is being appended to next listing. FIX!\n",
    "        # update: fixed.\n",
    "        area = None\n",
    "        try:\n",
    "            l_details = l.find_element_by_class_name('details_info').find_elements_by_tag_name('span')\n",
    "            for detail in l_details:\n",
    "                 if detail.text.find('ft') > 0:\n",
    "                    area = float(detail.text.split(' ')[0].replace(',', ''))\n",
    "            if area:\n",
    "                sq_area.append(area)\n",
    "            else:\n",
    "                sq_area.append('N/A')\n",
    "        except:\n",
    "            sq_area.append('N/A')\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # listing type and neighborhood\n",
    "        l_type = None\n",
    "        nhood = None\n",
    "        try:\n",
    "            area_details = l.find_elements_by_class_name('details_info')[1].text\n",
    "            l_type, nhood = area_details.split(' in ')\n",
    "            if l_type:\n",
    "                listing_type.append(l_type)\n",
    "            else:\n",
    "                listing_type.append('N/A')\n",
    "            if nhood:\n",
    "                n_hood.append(nhood)\n",
    "            else:\n",
    "                n_hood.append('N/A')\n",
    "        except:\n",
    "            listing_type.append('N/A')\n",
    "            n_hood.append('N/A')\n",
    "        # time.sleep(1)\n",
    "    # if count == 14:\n",
    "        # print('Moving on to the next page...')\n",
    "    # streeteasy introduces a captcha when they suspect scraping. How will this be overridden?\n",
    "    # fixed by using Firefox in place of Chrome\n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_page():\n",
    "    nxt = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CLASS_NAME, 'next')))\n",
    "    # nxt = listns[-1].find_element_by_class_name('next')\n",
    "    nxt.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "driver = webdriver.Firefox()\n",
    "for i in range(0, len(urls)):\n",
    "    counter = 1\n",
    "    driver.get(urls[i])\n",
    "    time.sleep(2)\n",
    "    listns = page_scrape(driver)\n",
    "    last_page = int(listns[-1].find_elements_by_class_name('page')[-1].text)\n",
    "    # print 'Counter:', counter\n",
    "    next_page()\n",
    "    for i in range(1, last_page):\n",
    "        counter += 1\n",
    "        b_time = time.time()\n",
    "        delay = random.uniform(2.5, 5)\n",
    "        listns = page_scrape(driver)\n",
    "        next_page()\n",
    "        # print 'Counter:', counter\n",
    "        time.sleep(delay)\n",
    "    print time.time() - b_time\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I need to figure out a way to stop page timeout @ 300000ms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>area</th>\n",
       "      <th>baths</th>\n",
       "      <th>beds</th>\n",
       "      <th>building type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6 Cortlandt Alley #5FL</td>\n",
       "      <td>3641</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4</td>\n",
       "      <td>Condop</td>\n",
       "      <td>40.7176</td>\n",
       "      <td>-74.0029</td>\n",
       "      <td>Tribeca</td>\n",
       "      <td>6.75e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301 West 53rd Street #22J</td>\n",
       "      <td>678</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Condo</td>\n",
       "      <td>40.7646</td>\n",
       "      <td>-73.9849</td>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>1.235e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55 W 95th Street #65</td>\n",
       "      <td>2000</td>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Condo</td>\n",
       "      <td>40.792</td>\n",
       "      <td>-73.9669</td>\n",
       "      <td>Upper West Side</td>\n",
       "      <td>3.25e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>390 West End Avenue #PHK</td>\n",
       "      <td>623</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Condo</td>\n",
       "      <td>40.7837</td>\n",
       "      <td>-73.9809</td>\n",
       "      <td>Upper West Side</td>\n",
       "      <td>999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>350 West 42nd Street #24J</td>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Condo</td>\n",
       "      <td>40.7579</td>\n",
       "      <td>-73.9923</td>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     address  area baths beds building type latitude  \\\n",
       "0     6 Cortlandt Alley #5FL  3641   4.5    4        Condop  40.7176   \n",
       "1  301 West 53rd Street #22J   678     1    1         Condo  40.7646   \n",
       "2       55 W 95th Street #65  2000     4  N/A         Condo   40.792   \n",
       "3   390 West End Avenue #PHK   623     1    1         Condo  40.7837   \n",
       "4  350 West 42nd Street #24J   525     1  N/A         Condo  40.7579   \n",
       "\n",
       "  longitude     neighborhood      price  \n",
       "0  -74.0029          Tribeca   6.75e+06  \n",
       "1  -73.9849   Hell's Kitchen  1.235e+06  \n",
       "2  -73.9669  Upper West Side   3.25e+06  \n",
       "3  -73.9809  Upper West Side     999999  \n",
       "4  -73.9923   Hell's Kitchen     830000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'building type':listing_type, 'latitude':lat, 'longitude':lng, 'address':address, 'beds':no_of_beds, 'baths':no_of_baths, 'area':sq_area, 'neighborhood':n_hood, 'price':price}\n",
    "data = pd.DataFrame(dic)\n",
    "# data.to_csv('Streeteasy_data.csv') \"The data is not yet good enough to warrant saving to disk...\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6748 entries, 0 to 6747\n",
      "Data columns (total 9 columns):\n",
      "address          6748 non-null object\n",
      "area             6748 non-null object\n",
      "baths            6711 non-null object\n",
      "beds             6748 non-null object\n",
      "building type    6748 non-null object\n",
      "latitude         6748 non-null object\n",
      "longitude        6748 non-null object\n",
      "neighborhood     6748 non-null object\n",
      "price            6748 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 474.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           address  area   baths    beds building type  \\\n",
      "count                         6748  6748  6711.0  6748.0          6748   \n",
      "unique                        5887  1604    23.0    19.0             8   \n",
      "top     303 West 66th Street #20LW   N/A     1.0     2.0         Condo   \n",
      "freq                             7  2415  2825.0  2079.0          3317   \n",
      "\n",
      "           latitude    longitude     neighborhood      price  \n",
      "count   6748.000000  6748.000000             6748     6748.0  \n",
      "unique  1627.000000  1391.000000               55     1316.0  \n",
      "top       40.710175   -73.989098  Upper West Side  1995000.0  \n",
      "freq      42.000000    46.000000              594       70.0  \n"
     ]
    }
   ],
   "source": [
    "print data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling 'N/A' values and outliers...\n",
    "\n",
    "*Are samples with missing data discarded or replaced with the feature median? What is the norm as pertains to this situation...?*\n",
    "\n",
    "Are statistical outliers really outliers in this use case? (Yes/**No**)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# sns.set(style='whitegrid', context='notebook')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Equations\n",
    "- Multivariate Linear Regression:\n",
    " - $y = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum\\limits_{i=0}^{\\infty} w_ix_i = w^Tx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CClass in module numpy.lib.index_tricks object:\n",
      "\n",
      "class CClass(AxisConcatenator)\n",
      " |  Translates slice objects to concatenation along the second axis.\n",
      " |  \n",
      " |  This is short-hand for ``np.r_['-1,2,0', index expression]``, which is\n",
      " |  useful because of its common occurrence. In particular, arrays will be\n",
      " |  stacked along their last axis after being upgraded to at least 2-D with\n",
      " |  1's post-pended to the shape (column vectors made out of 1-D arrays).\n",
      " |  \n",
      " |  For detailed documentation, see `r_`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
      " |  array([[1, 2, 3, 0, 0, 4, 5, 6]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CClass\n",
      " |      AxisConcatenator\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from AxisConcatenator:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __getslice__(self, i, j)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from AxisConcatenator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "help(np.c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure\n",
    "\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
